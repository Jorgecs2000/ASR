# Práctica 6 
Autor: Jorge Calvo Soria

# Creación del Cluster
En esta primera parte se configura un cluster de k8s, que estará sirviendo a un servicio web apache.
1.  Primero realizamos gcloud init y elegimos el proyecto.
2.  Establecemos la zona del proyecto.

    ![](media/90f57e796973cae148270310adb6af7a.png)


3.  Previamente hemos establecido una configuración previa en un archivo "config.ini". Tras dicha configuración
 ``` ini
 zone="europe-west1-b"
cluster_name="scaling-demo"
php_deployment="php-apache"
php_manifest="php-apache.yaml"
min_replicas=1
max_replicas=10
cpu_threshold=50
min_nodes=1
max_nodes=5
```
4. Se inicia el cluster de kubernetes en Google con el siguiente comando

    ![A computer screen capture Description automatically generated with medium confidence](media/22952a9f40fabaa2626d1ed21e0fce4b.png)

5. Para demostrar el autoescalado horizontal de pods vamos a desplegar una imagen de docker basada en php-apache.Para esto, vamos a hacer uso de un manifiesto de despliegue, php-apache.yaml, que contiene la siguiente configuración:

``` yaml
 apiVersion: apps/v1
kind: Deployment
metadata:
  name: php-apache
spec:
  selector:
    matchLabels:
      run: php-apache
  replicas: 3
  template:
    metadata:
      labels:
        run: php-apache
    spec:
      containers:
      - name: php-apache
        image: k8s.gcr.io/hpa-example
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 500m
          requests:
            cpu: 200m
---
apiVersion: v1
kind: Service
metadata:
  name: php-apache
  labels:
    run: php-apache
spec:
  ports:
  - port: 80
  selector:
    run: php-apache
```

6.  Aplicamos el manifiesto

    ![](media/c092214e7e29a99cbdd821f1c3275802.png)

7.  Mostramos el comportamiento del clusterinspeccionando los despliegues que se encuentran en funcionamiento en nuestro cluster
    ![](media/0024ad064842103e467cdf9310808d8f.png)

6.  Aplicamos la configuración del HPA

    ![](media/41a969499184c492bb18d4cb071c83ed.png)

7.  Comprobamos el estado del HPA

    ![](media/da473b9201c5f746afe93d8479c1f172.png)

8.  Activamos el autoescalado horizontal del cluster

    ![](media/38853c30d899803168e50a6c79afdeb5.png)

9.  cambiamos el perfil del autoescalado introduciendo "optimaze-utilization"

    ![](media/0d5c6d694d05cf6cc87897d936c8e96a.png)

10. Nodos disponibles que podemos usar:

    ![](media/701e511320f1d3aaffb847769060aeed.png)

# Entrega 2

1.  En el directorio donde se encuentra el DockerFile creamos la imagen. Usamos el comando -\> “sudo docker build -t gcr.io/pract6-366611/ab:v0.0.1 .”

    ![Text Description automatically generated](media/f9b7df5ae2d606e61dd581cd0d8f54c9.png)

    DockerFile

    ![Text Description automatically generated](media/abc5a2e1537782fb4c1349c97288c485.png)

2.  Pusheamos a Google la imagen con el comando “gcloud builds submit --tag gcr.io/pract6-366611/ab:v0.0.1 .”
3.  Creamos el job.yaml

    ![Text, application Description automatically generated](media/23bd4b971f7a50bfc0c278eab93ec6a1.png)

4.  Comprobamos que funciona:

    ![](media/1a703f5b8fb1c50e519d3e715acd321d.png)

5.  Cluster creado en Google:

    ![Graphical user interface, text, application, email Description automatically generated](media/ed6898d2a8b87e3b15f48184103aaa08.png)

6.  Usando Lens sacamos información relevante sobre el cluster:

    ![A screenshot of a computer Description automatically generated](media/6d06e1fa96d81068968051cd8515bd7d.png)

# Entrega 3

1.  Se crea el cluster igual que en la entrega 2 para un nuevo proyecto

Previamente a empezar con esta entrega hay que habilitar Cloud Build Kubernetes Engine Google App Engine Admin API Cloud Storage.

2.  Construimos y subimos la imagen de Docker a gcloud

    ![A picture containing timeline Description automatically generated](media/fb71ba85edcbb38eefce791d487ca131.png)

3.  Cambiamos el [Target Host] -> http://php-apache y el nombre del proyecto id del proyecto de los archivos locust-master-controller.yaml y locust-worker-controller.yaml

    Master Controler

    ![Table Description automatically generated with medium confidence](media/7e9d64c054dd5febbf5af3e996fdf764.png)

    Worker controller

    ![Table Description automatically generated with low confidence](media/e30a5659799353d742537151fca22260.png)

4.  Levantamos los nodos de locust master y locust worker

    ![A picture containing text Description automatically generated](media/6ab47f5eaef3f1d96edb69cee14955dc.png)

5.  Obtenemos la dirección IP externa del servicio locust master y la introducimos en el buscador.

    ![](media/277d491f676b5d2d64923e2a01322738.png)

    ![Graphical user interface, website Description automatically generated](media/78baf325f33682aa5565b18f7350d571.png)

    En un primer caso para 10 pods máximos establezco 10000 users y un Hatch rate de 1000.

    ![Graphical user interface, chart Description automatically generated](media/aba14b750945814b281b065db3abc17f.png)

    ![A screenshot of a computer Description automatically generated](media/c216eefbdf5f160dae2041c1a3324abf.png)

    Para otro caso de 10 pods máximos establezco 10 users y un Hatch rate de 2.

![A screenshot of a computer Description automatically generated](media/cf2b99019137e4bb9789c6931478ef08.png)

![Text Description automatically generated](media/1a5d0dfc14a52ddc06efad517dca2257.png)

Ahora levanto otra vez el cluster y cambio el número máximo de réplicas

![](media/3319ef743cfeb945d3675912ecc4fdd0.png)

![](media/4ad018ad4500b9e45f28a7ae3dcba058.png)

Introducimos 2000 ususarios y Hatch rate de 100 con un máximo de 50 pods

![A screenshot of a computer Description automatically generated](media/d2d073a9b8ee9888ef051e5c4f026bbf.png)

![A screenshot of a computer Description automatically generated with medium confidence](media/461bb2c95532daf78fd6dab4379a0d00.png)

![](media/faaec5cbcbed00762cc91326c349173c.png)

Se llega a la conclusión de que a cuantos más pods se usan más request por segundo (RPS).

Además, también disminuye el porcentaje de error cuando el número de pods es más alto.